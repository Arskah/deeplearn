{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "727032c607fcb1a2a20d18a7157605f2",
     "grade": false,
     "grade_id": "cell-f5e46023398b0aab",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 8\n",
    "<br>\n",
    "<b>Deadline:</b> April 7, 2021 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 6. Neural machine translation with transformers.\n",
    "\n",
    "The goal of this exerscise is to get familiar with a transformer model, which was introduced in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf).\n",
    "\n",
    "We base our code on the [Annotated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) blog post.\n",
    "Module `transformer.py` contains some useful modules from that blog post. We recommend you to use those modules when we state so in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformer as tr\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is ../data\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbbca8fe9cf0cb1cb20dd200e23cfcb0",
     "grade": false,
     "grade_id": "cell-44cf6f3242607cde",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7fd231dc4a3319981678549c38c5d02",
     "grade": false,
     "grade_id": "cell-1f1e529682d7ce6d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We use the same translation dataset as in Exercise 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16144210eada0de3091243f95367f7c2",
     "grade": false,
     "grade_id": "cell-94d57799bcd1786b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence:\n",
      " as word indices:  tensor([ 118,  215,  244, 1311,    5,    1])\n",
      " as string:  vous etes mon heroine . EOS\n",
      "Target sentence:\n",
      " as word indices:  tensor([130, 125, 146,  86,   4,   1])\n",
      " as string:  you are my hero . EOS\n"
     ]
    }
   ],
   "source": [
    "# Translation data\n",
    "from data import TranslationDataset, SOS_token, EOS_token, MAX_LENGTH\n",
    "trainset = TranslationDataset(data_dir, train=True)\n",
    "\n",
    "src_seq, tgt_seq = trainset[np.random.choice(len(trainset))]\n",
    "print('Source sentence:')\n",
    "print(' as word indices: ', src_seq)\n",
    "print(' as string: ', ' '.join(trainset.input_lang.index2word[i.item()] for i in src_seq))\n",
    "\n",
    "print('Target sentence:')\n",
    "print(' as word indices: ', tgt_seq)\n",
    "print(' as string: ', ' '.join(trainset.output_lang.index2word[i.item()] for i in tgt_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48ecf9ce3f0b267a5c738cc20ab96250",
     "grade": false,
     "grade_id": "cell-86482ed71ea81ed3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Custom DataLoader\n",
    "\n",
    "Next we prepare a custom data loader which puts sequences of varying lengths in one tensor. We do so by using a custom `collate_fn` (see the description of the `collate_fn` argument of [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)).\n",
    "\n",
    "Our collate function combines source sequences in one tensor `src_seqs` with extra values (at the end) filled with `PADDING_VALUE=0`. To tell the transformer which elements are padded, we also need to compute the mask `src_mask`.\n",
    "\n",
    "The function also combines target sequences in one tensor `tgt_seqs` but it does it a bit differently:\n",
    "* The resulting tensor is of shape `(max_tgt_seq_length+1, batch_size)`, where `max_tgt_seq_length` is the length of the longest target sequence in the mini-batch.\n",
    "* The first element of each sequence in the resulting tensor is `SOS_token`.\n",
    "* The remaining elements are filled similarly to the source sequences with extra values (at the end) filled with `PADDING_VALUE=0`.\n",
    "\n",
    "We will use tensor `tgt_seqs[:-1]` as inputs of the transformer decoder and `tgt_seqs[1:]` as the targets for the model (decoder) outputs. The `SOS_token` is needed to predict the first word in the output sequence in the corresponding (first) location of the decoder output.\n",
    "\n",
    "Your task is to implement this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "364bdf1c40bbfd85d74a002df498033f",
     "grade": false,
     "grade_id": "cell-77f035ba6ccb554d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d80be252d280552db0469e868f6c2ed0",
     "grade": false,
     "grade_id": "collate",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "    \"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "    Args:\n",
    "      list_of_samples is a list of tuples (src_seq, tgt_seq):\n",
    "          src_seq is of shape (src_seq_length)\n",
    "          tgt_seq is of shape (tgt_seq_length)\n",
    "\n",
    "    Returns:\n",
    "      src_seqs of shape (max_src_seq_length, batch_size): LongTensor of padded source sequences.\n",
    "      src_mask of shape (max_src_seq_length, batch_size): BoolTensor (tensor with boolean elements) indicating which\n",
    "          elements of the src_seqs tensor should be ignored in computations: True values in src_mask correspond\n",
    "          to padding values in src_seqs.\n",
    "      tgt_seqs of shape (max_tgt_seq_length+1, batch_size): LongTensor of padded target sequences.\n",
    "    \"\"\"\n",
    "    src = [item[0] for item in list_of_samples]\n",
    "    tgt = [item[1] for item in list_of_samples]\n",
    "    \n",
    "    src =  pad_sequence(src, padding_value=PADDING_VALUE)\n",
    "    src_mask = (src == PADDING_VALUE)\n",
    "\n",
    "    tgt = pad_sequence(tgt, padding_value=PADDING_VALUE)    \n",
    "    sos = torch.LongTensor([[SOS_token]* tgt.shape[1]])\n",
    "    tgt = torch.cat((sos, tgt), 0)\n",
    "    \n",
    "    return src, src_mask, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ce1cbec7ff20d420fc4a68362c5e481",
     "grade": false,
     "grade_id": "cell-a99b4cfa1fc559f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_collate_shapes():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([2, EOS_token]), torch.LongTensor([3, 4, EOS_token])),\n",
    "        (torch.LongTensor([6, 7, EOS_token]), torch.LongTensor([9, EOS_token])),\n",
    "    ]\n",
    "    src_seqs, src_mask, tgt_seqs = collate(pairs)\n",
    "    assert src_seqs.dtype == torch.long, f\"Wrong src_seqs.dtype: {src_seqs.dtype}\"\n",
    "    assert src_seqs.shape == torch.Size([3, 2]), f\"Wrong src_seqs.shape: {src_seqs.shape}\"\n",
    "\n",
    "    assert tgt_seqs.dtype == torch.long, f\"Wrong tgt_seqs.dtype: {tgt_seqs.dtype}\"\n",
    "    assert tgt_seqs.shape == torch.Size([4, 2]), f\"Wrong tgt_seqs.shape: {tgt_seqs.shape}\"\n",
    "    assert (tgt_seqs[0] == torch.empty(2, dtype=torch.long).fill_(SOS_token)).all(), \"Target sequences should start with SOS_token.\"\n",
    "    \n",
    "    assert src_mask.dtype == torch.bool, f\"Wrong src_mask.dtype: {src_mask.dtype}\"\n",
    "    assert src_mask.shape == src_seqs.shape, f\"Wrong src_mask.shape: {src_mask.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_collate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecb163890aeafc6188b0431e322b20c7",
     "grade": true,
     "grade_id": "test_collate",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests collate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f103d72cfd56ec7f6e42872740b6f395",
     "grade": false,
     "grade_id": "cell-e0a6bbaf21ae2a36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=64, shuffle=True, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6627cb26c83634c853b5749710275900",
     "grade": false,
     "grade_id": "cell-3f6dfc8dc7015270",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59db1cacb16c490534b678abc3ffefce",
     "grade": false,
     "grade_id": "cell-63be98428fcdc0b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Encoder block\n",
    "\n",
    "<img src=\"encoder_block.png\" width=150 style=\"float: right;\">\n",
    "\n",
    "We first implement one block of the transformer encoder (see the figure on the right).\n",
    "* We recommend you to use layers available in PyTorch:\n",
    "  * [nn.LayerNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) to implement the `Norm` layer in the figure\n",
    "  * [nn.Dropout](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) to implement dropout\n",
    "  * [nn.MultiheadAttention](https://pytorch.org/docs/stable/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention) to implement `Multi-Head Attention`.\n",
    "\n",
    "* `Feedforward` is simply an MLP processing each position (each element of the source sequence) independently. The exact implementation of the MLP is not tested in this notebook. We used an MLP with:\n",
    "  * one hidden layer with `n_hidden` neurons\n",
    "  * a dropout and ReLU activation after the hidden layer\n",
    "  * an output layer with `n_features` outputs.\n",
    "\n",
    "* We used dropout in both skip connections of the encoder block.\n",
    "* In two places where skip connections are used, we applied dropout on the main path, combined the main path with the skip connection and then applied layer normalization. This order is slightly different to the [Annotated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) code.\n",
    "\n",
    "Hints:\n",
    "* **We recommend you to test that the padded values of the input sequence do not affect the outputs in the positions that correspond to non-padded values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d490069146fdfa32112d33744eb59d1",
     "grade": false,
     "grade_id": "EncoderBlock",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, dropout):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_inputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_features: Number of input and output features.\n",
    "          n_heads: Number of attention heads in the Multi-Head Attention.\n",
    "          n_hidden: Number of hidden units in the Feedforward (MLP) block.\n",
    "          dropout: Dropout rate after the first layer of the MLP and in two places on the main path (before\n",
    "                   combining the main path with a skip connection).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm_1 = nn.LayerNorm(n_features)\n",
    "        self.norm_2 = nn.LayerNorm(n_features)\n",
    "        self.attn = nn.MultiheadAttention(n_features, n_heads) #, dropout=dropout\n",
    "        self.ff = FeedForward(n_features, n_hidden, dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (max_seq_length, batch_size, n_features): Input sequences.\n",
    "          mask of shape (max_seq_length, batch_size): BoolTensor indicating which elements of the input\n",
    "              sequences should be ignored (True values correspond to ignored elements in x).\n",
    "        \n",
    "        Returns:\n",
    "          z of shape (max_seq_length, batch_size, n_features): Encoded input sequences.\n",
    "\n",
    "        Note: All intermediate signals should be of shape (max_seq_length, batch_size, n_features).\n",
    "        \"\"\"\n",
    "        # Flip mask dimensions\n",
    "        mask = mask.transpose(0, 1)\n",
    "        \n",
    "        out2 = self.attn(x, x, x, key_padding_mask=mask)[0]\n",
    "        out = x + self.dropout_1(out2)\n",
    "        out = self.norm_1(out)\n",
    "        out2 = self.ff(out)\n",
    "        out = out + self.dropout_2(out2)\n",
    "        out = self.norm_2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0146983a24b7930bef1ba41bd0c3d4e",
     "grade": false,
     "grade_id": "cell-67f5cb6fdfecf7d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_EncoderBlock_shapes():\n",
    "    encoder_block = EncoderBlock(n_features=16, n_heads=4, n_hidden=64)\n",
    "\n",
    "    x = torch.tensor([\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 0],\n",
    "        [6, 0],\n",
    "    ]).float().view(4, 2, 1).repeat(1, 1, 16)  # (max_seq_length, batch_size, n_features)\n",
    "\n",
    "    mask = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "    ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "    outputs = encoder_block(x, mask)\n",
    "    assert outputs.shape == torch.Size([4, 2, 16]), f\"Wrong outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_EncoderBlock_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c87d80ba7f699f09aedf9c4a27d45c34",
     "grade": true,
     "grade_id": "test_EncoderBlock",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests EncoderBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b121062b3d9db25b2cf16db2bc28af9",
     "grade": false,
     "grade_id": "cell-e2776b50381263dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"encoder.png\" width=200 style=\"float: right;\">\n",
    "\n",
    "## Encoder\n",
    "\n",
    "The encoder is a stack of the following blocks:\n",
    "* Embedding of words (please use [nn.Embedding](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding))\n",
    "* Positional encoding (please use `tr.PositionalEncoding` from the attached module)\n",
    "* `n_blocks` of the `EncoderBlock` modules.\n",
    "\n",
    "Notes:\n",
    "* Provided implementation of `tr.PositionalEncoding` is the same as in [Annotated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) blog post. See the blog post for more detail.\n",
    "* Our longest sequences have length `MAX_LENGTH`, this is the value that you can use when you specify `PositionalEncoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50bf400c4630234e0437a7c18c884abd",
     "grade": false,
     "grade_id": "Encoder",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, n_blocks, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          src_vocab_size: Number of words in the source vocabulary.\n",
    "          n_blocks: Number of EncoderBlock blocks.\n",
    "          n_features: Number of features to be used for word embedding and further in all layers of the encoder.\n",
    "          n_heads: Number of attention heads inside the EncoderBlock.\n",
    "          n_hidden: Number of hidden units in the Feedforward block of EncoderBlock.\n",
    "          dropout: Dropout level used in EncoderBlock.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(src_vocab_size, n_features)\n",
    "        self.pe = tr.PositionalEncoding(n_features, dropout=dropout, max_len=MAX_LENGTH)\n",
    "        self.layers = nn.ModuleList([EncoderBlock(n_features, n_heads, n_hidden, dropout) for i in range(n_blocks)])\n",
    "        self.norm = nn.LayerNorm(n_features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (max_seq_length, batch_size): LongTensor with the input sequences.\n",
    "          mask of shape (max_seq_length, batch_size): BoolTensor indicating which elements should be ignored.\n",
    "        \n",
    "        Returns:\n",
    "          z of shape (max_seq_length, batch_size, n_features): Encoded input sequences.\n",
    "\n",
    "        Note: All intermediate signals should be of shape (max_seq_length, batch_size, n_features).\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        assert out.isnan().any() == False\n",
    "        out = self.pe(out)\n",
    "        assert out.isnan().any() == False\n",
    "        for func in self.layers:\n",
    "            out = func(out, mask)\n",
    "            assert out.isnan().any() == False\n",
    "        #return out\n",
    "        return self.norm(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84a652b200c4f5bbfa06a5c36071dd3c",
     "grade": false,
     "grade_id": "cell-01134ebab1f5117e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Encoder_shapes():\n",
    "    encoder = Encoder(src_vocab_size=10, n_blocks=1, n_features=16, n_heads=4, n_hidden=64)\n",
    "\n",
    "    x = torch.tensor([\n",
    "        [SOS_token,     SOS_token],\n",
    "        [        3,             4],\n",
    "        [        5,     EOS_token],\n",
    "        [        6, PADDING_VALUE],\n",
    "        [EOS_token, PADDING_VALUE],\n",
    "    ])  # (max_seq_length, batch_size)\n",
    "\n",
    "    mask = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "    ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "    outputs = encoder(x, mask)\n",
    "    assert outputs.shape == torch.Size([5, 2, 16]), f\"Wrong outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Encoder_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c59d3e8bf698511a855df8b7fbc04491",
     "grade": false,
     "grade_id": "cell-102038af7faba64b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aea4e7f9885ee84d12e6ab183ffa4c45",
     "grade": false,
     "grade_id": "cell-3133e50590987e56",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Subsequent mask\n",
    "\n",
    "In the training loop, we will use target sequences (starting with `SOS_token`) as inputs of the decoder. By doing that, we make it possible for the decoder to use previously decoded words when predicting probabilities of the next word. This idea is similar to the way decoding was done in Exercise 5. However, the computations are parallelized in the transformer decoder, and the probabilities of each word in the target sequence are produced by doing one pass through the decoder.\n",
    "\n",
    "During decoding, we need to make sure that when we compute the probability of the next word, we only use preceding and not subsequent words. In transformers, this is done by providing a mask which tells which elements should be used or ignored when producing the output. The following function produces this kind of mask.\n",
    "\n",
    "The $i$-th row in the produced mask says which of the input elements should be used to compute the $i$-th element of the output:\n",
    "* `0`: the corresponding element of the input sequence can be used.\n",
    "* `-inf`: the corresponding element of the input sequence cannot be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c91dbc90b6c803baeffb280bbf39324b",
     "grade": false,
     "grade_id": "cell-c2dfee9a9f2d674e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1).float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20d6d1ff1df0c0609a1cfc1954e4e94c",
     "grade": false,
     "grade_id": "cell-ec36a7f1884b54f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14577b9d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is a typical mask that we need to use while decoding\n",
    "mask = subsequent_mask(10)\n",
    "print(mask)\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8db968d0d9c4b5706ab23917e54237f",
     "grade": false,
     "grade_id": "cell-c26c4a8fecf141bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"decoder_block.png\" width=150 style=\"float: right;\">\n",
    "\n",
    "## Decoder block\n",
    "\n",
    "Next we implement one block of the transformer decoder (see the figure on the right).\n",
    "* We recommend you to use layers available in PyTorch:\n",
    "  * [nn.LayerNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) to implement the `Norm` layer in the figure\n",
    "  * [nn.Dropout](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) to implement dropout\n",
    "  * [nn.MultiheadAttention](https://pytorch.org/docs/stable/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention) to implement `Multi-Head Attention`.\n",
    "\n",
    "* `Feedforward` is simply an MLP processing each position (each element of the source sequence) independently. The exact implementation of the MLP is not tested in this notebook. We used an MLP with:\n",
    "  * one hidden layer with `n_hidden` neurons\n",
    "  * a dropout and ReLU activation after the hidden layer\n",
    "  * an output layer with `n_features` outputs.\n",
    "\n",
    "* In three places where skip connections are used, we applied dropout on the main path, combined the main path with the skip connection and then applied layer normalization. This order is slightly different to the [Annotated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) code.\n",
    "\n",
    "Notes:\n",
    "* The first attention block is self-attention when query, key and value inputs are same. The second attention block uses the encoded `z` values as keys and values, and the outputs of the previous layer as query.\n",
    "* **We recommend you to test that the subsequent values of the input sequence do not affect the outputs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c362c463285842998fa3f021db9c3fd4",
     "grade": false,
     "grade_id": "DecoderBlock",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_features: Number of input and output features.\n",
    "          n_heads: Number of attention heads in the Multi-Head Attention.\n",
    "          n_hidden: Number of hidden units in the Feedforward (MLP) block.\n",
    "          dropout: Dropout rate after the first layer of the MLP and in three places on the main path (before\n",
    "                   combining the main path with a skip connection).\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(n_features, n_heads) #, dropout=dropout\n",
    "        self.multihead_attn = nn.MultiheadAttention(n_features, n_heads) #, dropout=dropout\n",
    "        self.ff = FeedForward(n_features, n_hidden, dropout)\n",
    "        self.norm1 = nn.LayerNorm(n_features)\n",
    "        self.norm2 = nn.LayerNorm(n_features)\n",
    "        self.norm3 = nn.LayerNorm(n_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, z, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          tgt of shape (max_tgt_seq_length, batch_size, n_features): Transformed target sequences used as the inputs\n",
    "              of the block.\n",
    "          z of shape (max_src_seq_length, batch_size, n_features): Encoded source sequences (outputs of the\n",
    "              encoder).\n",
    "          src_mask of shape (max_src_seq_length, batch_size): BoolTensor indicating which elements of the\n",
    "             encoded source sequences should be ignored.\n",
    "          tgt_mask of shape (max_tgt_seq_length, max_tgt_seq_length): Subsequent mask to ignore subsequent\n",
    "             elements of the target sequences in the inputs. The rows of this matrix correspond to the output\n",
    "             elements and the columns correspond to the input elements.\n",
    "        \n",
    "        Returns:\n",
    "          out of shape (max_seq_length, batch_size, n_features): Output tensor.\n",
    "\n",
    "        Note: All intermediate signals should be of shape (max_seq_length, batch_size, n_features).\n",
    "        \"\"\"\n",
    "        # Flip mask dimensions\n",
    "        src_mask = src_mask.transpose(0, 1)\n",
    "\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)        \n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt, z, z, key_padding_mask=src_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.ff(tgt)\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9b349c444881b030f492a713a0ade9d",
     "grade": false,
     "grade_id": "cell-586469eac0b19254",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_DecoderBlock_shapes():\n",
    "    decoder_block = DecoderBlock(n_features=16, n_heads=4, n_hidden=64)\n",
    "\n",
    "    y = torch.tensor([\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 0],\n",
    "        [6, 0],\n",
    "    ]).float().view(4, 2, 1).repeat(1, 1, 16)  # (max_seq_length, batch_size, n_features)\n",
    "\n",
    "    z = torch.randn(4, 2, 16, requires_grad=True)  # (max_seq_length, batch_size, n_features)\n",
    "\n",
    "    src_mask = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "    ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "\n",
    "    tgt_mask = subsequent_mask(y.size(0))\n",
    "\n",
    "    outputs = decoder_block(y, z, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "    assert outputs.shape == torch.Size([4, 2, 16]), f\"Wrong outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_DecoderBlock_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd5a1efcc88dc360d0259d3387b9a826",
     "grade": true,
     "grade_id": "test_DecoderBlock",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests DecoderBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4af57790480337233da05440de8d44c",
     "grade": false,
     "grade_id": "cell-a30448f3b22189c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"decoder.png\" width=200 style=\"float: right;\">\n",
    "\n",
    "## Decoder\n",
    "\n",
    "The decoder is a stack of the following blocks:\n",
    "* Embedding of words (please use [nn.Embedding](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding))\n",
    "* Positional encoding (please use `tr.PositionalEncoding` from the attached module)\n",
    "* `n_blocks` of the `DecoderBlock` modules.\n",
    "* A linear layer with `tgt_vocab_size` output features.\n",
    "* Log_softmax nonlinearity.\n",
    "\n",
    "Note: our longest sequences have length `MAX_LENGTH`, this is the value that you can use when you specify `PositionalEncoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1b8877ee3226b22ab4145c196d6f50d",
     "grade": false,
     "grade_id": "Decoder",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, n_blocks, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          tgt_vocab_size: Number of words in the target vocabulary.\n",
    "          n_blocks: Number of EncoderBlock blocks.\n",
    "          n_features: Number of features to be used for word embedding and further in all layers of the decoder.\n",
    "          n_heads: Number of attention heads inside the DecoderBlock.\n",
    "          n_hidden: Number of hidden units in the Feedforward block of DecoderBlock.\n",
    "          dropout: Dropout level used in DecoderBlock.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(tgt_vocab_size, n_features)\n",
    "        self.pe = tr.PositionalEncoding(n_features, dropout=dropout, max_len=MAX_LENGTH)\n",
    "        self.layers = nn.ModuleList([DecoderBlock(n_features, n_heads, n_hidden, dropout) for i in range(n_blocks)])\n",
    "        self.linear = nn.Linear(n_features, tgt_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, y, z, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          y of shape (max_tgt_seq_length, batch_size): LongTensor with the target sequences.\n",
    "          z of shape (max_src_seq_length, batch_size, n_features): Encoded source sequences (outputs of the\n",
    "              encoder).\n",
    "          src_mask of shape (max_src_seq_length, batch_size): Boolean tensor indicating which elements of the\n",
    "             source sequences should be ignored.\n",
    "        \n",
    "        Returns:\n",
    "          out of shape (max_seq_length, batch_size, tgt_vocab_size): Log-softmax probabilities of the words\n",
    "              in the output sequences.\n",
    "\n",
    "        Notes:\n",
    "          * All intermediate signals should be of shape (max_seq_length, batch_size, n_features).\n",
    "          * You need to create and use the subsequent mask in the decoder.\n",
    "        \"\"\"\n",
    "        tgt_mask = (y != PADDING_VALUE)\n",
    "        #sub_mask = torch.autograd.Variable(subsequent_mask(tgt_mask.shape[0]) == 0)\n",
    "        sub_mask = subsequent_mask(tgt_mask.shape[0])\n",
    "        tgt_mask = sub_mask\n",
    "        out = self.embed(y)\n",
    "        assert out.isnan().any() == False\n",
    "        out = self.pe(out)\n",
    "        assert out.isnan().any() == False\n",
    "        for func in self.layers:\n",
    "            out = func(out, z, src_mask, tgt_mask)\n",
    "            assert out.isnan().any() == False\n",
    "            \n",
    "        out = self.linear(out)\n",
    "        assert out.isnan().any() == False\n",
    "        out = self.softmax(out)\n",
    "        assert out.isnan().any() == False\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51bb46bd5ecbe26f8645207b8f0ff89b",
     "grade": false,
     "grade_id": "cell-2511aa618604b4a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Decoder_shapes():\n",
    "    decoder = Decoder(tgt_vocab_size=10, n_blocks=1, n_features=16, n_heads=4, n_hidden=64)\n",
    "\n",
    "    y = torch.tensor([\n",
    "        [SOS_token,     SOS_token],\n",
    "        [        3,             4],\n",
    "        [        5,     EOS_token],\n",
    "        [        6, PADDING_VALUE],\n",
    "        [        7, PADDING_VALUE],\n",
    "    ])  # (max_seq_length, batch_size)\n",
    "\n",
    "    z = torch.randn(5, 2, 16)  # (max_seq_length, batch_size, n_features)\n",
    "\n",
    "    src_mask = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "    ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "\n",
    "    outputs = decoder(y, z, src_mask=src_mask)\n",
    "    assert outputs.shape == torch.Size([5, 2, 10]), f\"Wrong outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Decoder_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac142e2969764fa1e646091bf0255a7a",
     "grade": false,
     "grade_id": "cell-10c28584fb58b386",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Train the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21ab3ba0364f3ec7e43d3204c702e831",
     "grade": false,
     "grade_id": "cell-ec608760ebfa7f8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embed): Embedding(2925, 256)\n",
       "  (pe): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): DecoderBlock(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): DecoderBlock(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=2925, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the transformer model\n",
    "n_features = 256\n",
    "encoder = Encoder(src_vocab_size=trainset.input_lang.n_words, n_blocks=3, n_features=n_features,\n",
    "                  n_heads=16, n_hidden=1024)\n",
    "decoder = Decoder(tgt_vocab_size=trainset.output_lang.n_words, n_blocks=3, n_features=n_features,\n",
    "                  n_heads=16, n_hidden=1024)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7452e92ad959e13ecdf0e16354afc06",
     "grade": false,
     "grade_id": "cell-b1645c0797a9d5f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "In the training loop, we first encode source sequences using the encoder. Then we decode the encoded sequences by the decoder which also receives shifted target sequences as inputs. The decoder outputs a tensor that contains log-softmax probabilities of words in the output language. You need to use those probabilities to compute the loss. Note that you need to ignore the padded values in the target sequences (similarly to Exercise 5).\n",
    "\n",
    "Hints:\n",
    "* The training loss should be smaller than 0.1 at the end of training.\n",
    "* If you use the `NoamOptimizer` defined below, you should reach the level of 0.1 after 15-20 epochs.\n",
    "* If you use the Adam optimizer with learning rate 0.001, you should reach the level of 0.1 after 40 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cc8a5cb93a4033853f0874b927adb57",
     "grade": false,
     "grade_id": "cell-3dcb8dddc9bd9ee7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "adam = torch.optim.Adam(parameters, lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer = tr.NoamOptimizer(n_features, 0.4, 680, adam)\n",
    "#optimizer = torch.optim.Adam(parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6270848f5387bf01aba9bb5f50303a78",
     "grade": false,
     "grade_id": "training_loop",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 4.286273688077927\n",
      "epoch: 2, loss: 2.1109261381275513\n",
      "epoch: 3, loss: 1.6127187758684158\n",
      "epoch: 4, loss: 1.2379825623596417\n",
      "epoch: 5, loss: 0.9500598714632147\n",
      "epoch: 6, loss: 0.7014569635776913\n",
      "epoch: 7, loss: 0.4906471251126598\n",
      "epoch: 8, loss: 0.3461321228567292\n",
      "epoch: 9, loss: 0.24742887278690057\n",
      "epoch: 10, loss: 0.18311141234110384\n",
      "epoch: 11, loss: 0.1456793523119653\n",
      "epoch: 12, loss: 0.1127606946427156\n",
      "epoch: 13, loss: 0.10518068880500163\n",
      "epoch: 14, loss: 0.0864533198011272\n",
      "epoch: 15, loss: 0.08340349245597334\n",
      "epoch: 16, loss: 0.06281013158150017\n",
      "epoch: 17, loss: 0.06508445064537227\n",
      "epoch: 18, loss: 0.06258058580843841\n",
      "epoch: 19, loss: 0.060012086185024065\n",
      "epoch: 20, loss: 0.054353841416099495\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implement the training loop in this cell\n",
    "if not skip_training:\n",
    "    epochs = 20    \n",
    "    total_loss = 0\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = []\n",
    "        for batch_num, training_batch in enumerate(trainloader):\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            src, src_mask, target = training_batch\n",
    "            src = src.to(device)\n",
    "            target = target.to(device)\n",
    "            src_mask = src_mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            tgt_input = target[:-1]\n",
    "            \n",
    "            # Ignore SOS token\n",
    "            targets = target[1:]\n",
    "            target_length = targets.shape[0]\n",
    "            \n",
    "            encoder_output = encoder(src, src_mask)\n",
    "            decoder_output = decoder(tgt_input, encoder_output, src_mask)\n",
    "            #print(decoder_output)\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            loss = 0\n",
    "            for i in range(target_length):\n",
    "                loss += F.nll_loss(decoder_output[i], targets[i], ignore_index=PADDING_VALUE)\n",
    "            loss = loss/target_length\n",
    "            loss_item = loss.item()\n",
    "            \n",
    "            #print(f\"loss: {loss_item}\")\n",
    "            running_loss += [loss_item]\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        mean_tot_loss = np.mean(running_loss)\n",
    "        print(f\"epoch: {epoch + 1}, loss: {mean_tot_loss}\")\n",
    "    plt.plot(running_loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 1_tr_encoder.pth.\n",
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 1_tr_decoder.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "# Set confirm=False if you do not want to be asked for confirmation before saving.\n",
    "if not skip_training:\n",
    "    tools.save_model(encoder, '1_tr_encoder.pth', confirm=True)\n",
    "    tools.save_model(decoder, '1_tr_decoder.pth', confirm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "623dde0b23a00fea2b9ede9fe041fade",
     "grade": false,
     "grade_id": "accuracy",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    encoder = Encoder(src_vocab_size=trainset.input_lang.n_words, n_blocks=3, n_features=256, n_heads=16, n_hidden=1024)\n",
    "    tools.load_model(encoder, '1_tr_encoder.pth', device)\n",
    "    \n",
    "    decoder = Decoder(tgt_vocab_size=trainset.output_lang.n_words, n_blocks=3, n_features=256, n_heads=16, n_hidden=1024)\n",
    "    tools.load_model(decoder, '1_tr_decoder.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3f81ca372cd2eb371bdc673917ee769",
     "grade": true,
     "grade_id": "cell-985f2404fb056035",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests the trained transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64256ad5b7d8e0aacd51b31d00e7e7b4",
     "grade": false,
     "grade_id": "cell-25e4072e5588afaa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Generate translations with the trained model\n",
    "\n",
    "In the cell below, implement a function that converts an input sequence to an output sequence using the trained transformer.\n",
    "\n",
    "Notes:\n",
    "* Since we do not need to compute the gradients in the evaluation phase, we can speed up the computations by using the statement `with torch.no_grad():`.\n",
    "* Please transfer the tensors to `device` inside this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6e4df363d49dedc5988600dae2a0c27",
     "grade": false,
     "grade_id": "cell-870f9d3b10a20a75",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, src_seqs, src_mask):\n",
    "    \"\"\"Translate sequences from the source language to the target language using the trained model.\n",
    "    \n",
    "    Args:\n",
    "      encoder (Encoder): Trained encoder.\n",
    "      decoder (Decoder): Trained decoder.\n",
    "      src_seqs of shape (max_src_seq_length, batch_size): LongTensor of padded source sequences.\n",
    "      src_mask of shape (max_src_seq_length, batch_size): BoolTensor indicating which elements of the src_seqs\n",
    "          tensor should be ignored in computations: True values in src_mask correspond to padding values in src_seqs.\n",
    "    \n",
    "    Returns:\n",
    "      out_seqs of shape (MAX_LENGTH, batch_size): LongTensor of word indices of the output sequences.\n",
    "      \n",
    "      NOTE: The SOS token should not be included in out_seqs.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0600455c19d0a266adaf7db9594964b5",
     "grade": true,
     "grade_id": "cell-ce3fec48785a469b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-3b45a36d7a97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Success'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtest_translate_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-3b45a36d7a97>\u001b[0m in \u001b[0;36mtest_translate_shapes\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msrc_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mout_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_seqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSOS_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"out_seqs should not include the SOS_token.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mout_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Wrong out_seqs.shape: {out_seqs.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-d9f44cc146d6>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(encoder, decoder, src_seqs, src_mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_translate_shapes():\n",
    "    src_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])\n",
    "    src_mask = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "    ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "\n",
    "    src_seqs, src_mask = src_seqs.to(device), src_mask.to(device)\n",
    "    out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "    assert not torch.any(out_seqs[0] == SOS_token), \"out_seqs should not include the SOS_token.\"\n",
    "    assert out_seqs.shape == torch.Size([MAX_LENGTH, 2]), f\"Wrong out_seqs.shape: {out_seqs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_translate_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a7476bd986bb058cb138bfba26f2f92",
     "grade": false,
     "grade_id": "cell-b57f4854a969cb31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below, we translate sentences from the training set. For a well-trained transformer, the translations should look similar to the target sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "303eed1d5a4185b59a3489fe57755342",
     "grade": false,
     "grade_id": "cell-a8ba1c2f1b1173fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def seq_to_tokens(seq, lang):\n",
    "    'Convert a sequence of word indices into a list of words (strings).'\n",
    "    sentence = []\n",
    "    for i in seq:\n",
    "        if i == EOS_token:\n",
    "            break\n",
    "        sentence.append(lang.index2word[i.item()])\n",
    "    return(sentence)\n",
    "\n",
    "def seq_to_string(seq, lang):\n",
    "    'Convert a sequence of word indices into a sentence string.'\n",
    "    return(' '.join(seq_to_tokens(seq, lang)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "213ac87cf5e0760ed26a57f7dca1a814",
     "grade": false,
     "grade_id": "cell-fb6fdac69a71a0c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Translate a few sentences from the training set\n",
    "print('Translate training data:')\n",
    "print('-----------------------------')\n",
    "src_seqs, src_mask, tgt_seqs = next(iter(trainloader))\n",
    "out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_string(src_seqs[:,i], trainset.input_lang))\n",
    "    print('TGT:', seq_to_string(tgt_seqs[1:,i], trainset.output_lang))\n",
    "    print('OUT:', seq_to_string(out_seqs[:,i], trainset.output_lang))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1d5f46da25dace4601c3cc25ef206da",
     "grade": false,
     "grade_id": "cell-67752c380a50b644",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below, we translate sentences from the test set. For a well-trained transformer, the translations are typically\n",
    "worse than for the training sequences but they still look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf39314da6c8979e1c74360dccf05b53",
     "grade": false,
     "grade_id": "cell-847fb19371903dba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "testset = TranslationDataset(data_dir, train=False)\n",
    "testloader = DataLoader(dataset=testset, batch_size=64, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b07b859592a4117a4b974375e3f3a46",
     "grade": false,
     "grade_id": "cell-6b598b0fd1633b71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Translate test data:')\n",
    "print('-----------------------------')\n",
    "src_seqs, src_mask, tgt_seqs = next(iter(testloader))\n",
    "out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_string(src_seqs[:,i], testset.input_lang))\n",
    "    print('TGT:', seq_to_string(tgt_seqs[1:,i], testset.output_lang))\n",
    "    print('OUT:', seq_to_string(out_seqs[:,i], testset.output_lang))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d43f85f04504c04acc69af1376f6b736",
     "grade": false,
     "grade_id": "cell-67c9d1b917f1f917",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Compute BLEU score\n",
    "\n",
    "Let us now compute the [BLEU score](https://en.wikipedia.org/wiki/BLEU) for the translations produced by our model. We can use the PyTorch function [bleu_score](https://pytorch.org/text/data_metrics.html#torchtext.data.metrics.bleu_score) for that.\n",
    "\n",
    "* **Your model should achive a BLEU score of 95 on the training set.**\n",
    "* The BLEU score on the test set can be significantly smaller (about 60).\n",
    "\n",
    "The model can severly overfit to the training set and we do not cope with the overfitting problem in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd58221014bd67cbf6347fd0b4d19a00",
     "grade": false,
     "grade_id": "cell-5a0ad89516fd02e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d31a3d639bd0b105c13a47eadfeb70ba",
     "grade": true,
     "grade_id": "cell-fa389a3327f82c0d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # Create translations for the training set\n",
    "    candidate_corpus = []\n",
    "    references_corpus = []\n",
    "    for src_seqs, src_mask, tgt_seqs in trainloader:\n",
    "        out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "        candidate_corpus.extend([seq_to_tokens(seq, trainset.output_lang) for seq in out_seqs.T])\n",
    "        references_corpus.extend([[seq_to_tokens(seq, trainset.output_lang)] for seq in tgt_seqs[1:].T])\n",
    "\n",
    "    # Compute BLEU for translations\n",
    "    score = bleu_score(candidate_corpus, references_corpus)\n",
    "    print(f'BLEU score on training data: {score*100}')\n",
    "    assert score*100 > 90, \"The BLEU score is too low.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa2b38fe546a0c57b1a48d376a64a8d4",
     "grade": false,
     "grade_id": "cell-49d0466ea4a34ba0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create translations for the test set\n",
    "candidate_corpus = []\n",
    "references_corpus = []\n",
    "for i, (src_seqs, src_mask, tgt_seqs) in enumerate(testloader):\n",
    "    out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "    candidate_corpus.extend([seq_to_tokens(seq, testset.output_lang) for seq in out_seqs.T])\n",
    "    references_corpus.extend([[seq_to_tokens(seq, testset.output_lang)] for seq in tgt_seqs[1:].T])\n",
    "    if i == 10:\n",
    "        break  # Use only 10 batches for testing\n",
    "\n",
    "# Compute BLEU for translations\n",
    "score = bleu_score(candidate_corpus, references_corpus)\n",
    "print(f'BLEU score on test data: {score*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c25aadf68f7f6983cd16a664ee1f44b7",
     "grade": false,
     "grade_id": "cell-14e56d480d46ea14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusion</b>\n",
    "</div>\n",
    "\n",
    "In this notebook:\n",
    "* We trained a transformer-based sequence-to-sequence model for statistical machine translation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
